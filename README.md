# ğŸ® RAWG Data Engineering Pipeline

Proyecto final de IngenierÃ­a de Datos que implementa un pipeline ETL robusto, modular e idempotente utilizando la **Arquitectura Medallion** (Bronze â¡ï¸ Silver) y **Delta Lake**.

---

## ğŸ“‹ DescripciÃ³n del Proyecto
Este sistema extrae datos de videojuegos desde la **RAWG Video Games API**, los almacena en su formato crudo (Bronze) y luego los limpia, transforma y enriquece para generar tablas analÃ­ticas (Silver).

### CaracterÃ­sticas Clave
- **Arquitectura Medallion:** SeparaciÃ³n clara entre datos crudos (`data/bronze`) y refinados (`data/silver`).
- **Almacenamiento Optimizado:** Uso de **Delta Lake** para ACID transactions y versionado.
- **Carga Incremental e Idempotente:**
  - El proceso de ingestiÃ³n garantiza que no se generen duplicados si se corre mÃºltiples veces el mismo dÃ­a (lÃ³gica *Delete-before-Write* por particiÃ³n).
- **Calidad de Datos:** Manejo robusto de esquemas, tipos de datos nulos y estructuras JSON anidadas.
- **Particionamiento:** Datos organizados por fecha de extracciÃ³n (`extraction_date`) para mejorar el rendimiento de consultas.

---

## ğŸ—ï¸ Estructura del Proyecto
```plaintext
TP-games/
â”œâ”€â”€ .env                  # Variables de entorno (API KEY) - NO COMMIT
â”œâ”€â”€ requirements.txt      # Dependencias del proyecto
â”œâ”€â”€ main.ipynb            # Orquestador del Pipeline (Jupyter Notebook)
â”œâ”€â”€ README.md             # DocumentaciÃ³n
â”œâ”€â”€ data/                 # Data Lake Local
â”‚   â”œâ”€â”€ bronze/           # Ingesta Cruda (JSON strings, particionado)
â”‚   â””â”€â”€ silver/           # Datos Transformados (Tablas limpias y KPIs)
â””â”€â”€ src/                  # CÃ³digo Fuente
    â”œâ”€â”€ config.py         # ConfiguraciÃ³n y validaciÃ³n de entorno
    â”œâ”€â”€ connectors.py     # Cliente API con Retries y Rate Limiting
    â”œâ”€â”€ ingestor.py       # LÃ³gica de extracciÃ³n (Bronze Layer)
    â””â”€â”€ transformer.py    # LÃ³gica de transformaciÃ³n (Silver Layer)
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Pre-requisitos
- Python 3.8+
- Una API Key de [RAWG](https://rawg.io/apidocs) (Gratuita).

### 2. Clonar e Instalar Dependencias
```bash
# Instalar librerÃ­as
pip install -r requirements.txt
```

### 3. Configurar Variables de Entorno
Crea un archivo `.env` en la raÃ­z del proyecto y agrega tu clave:
```ini
RAWG_API_KEY=tu_api_key_aqui
```

---

## â–¶ï¸ EjecuciÃ³n
El proyecto estÃ¡ orquestado a travÃ©s de **Jupyter Notebook**.

1. Abre `main.ipynb`.
2. Ejecuta todas las celdas secuencialmente.
3. El notebook realizarÃ¡:
   - **Full Load** de GÃ©neros.
   - **Incremental Load** de Juegos (Ãšltimos 30 dÃ­as).
   - **TransformaciÃ³n** a capa Silver.
   - **VerificaciÃ³n** mostrando los resultados finales.

---

## ğŸ› ï¸ Detalles TÃ©cnicos
- **Idempotencia:** En la carga incremental de juegos, el script elimina preventivamente los datos de la fecha actual antes de insertar los nuevos, permitiendo re-ejecuciones seguras.
- **Manejo de Esquemas:** Se utiliza serializaciÃ³n JSON segura para columnas complejas (listas de plataformas, gÃ©neros, etc.) para evitar errores de tipo en Delta Lake.

---

## ğŸ“Š Resultados (Silver Layer)
Al finalizar, encontrarÃ¡s dos tablas principales en `data/silver`:
1. **games_refined:** Tabla maestra de juegos limpia, deduplicada y tipada.
2. **games_analytics:** AgregaciÃ³n de mÃ©tricas (Rating Promedio y Cantidad de Juegos) por AÃ±o y GÃ©nero.

---

## ğŸ“ Comentarios Finales
Este proyecto fue desarrollado bajo estrictas limitaciones de tiempo. Como CientÃ­fico de Datos, el objetivo principal de cursar esta materia ha sido expandir mis capacidades en el Ã¡mbito de la IngenierÃ­a de Datos.

Si bien la implementaciÃ³n actual cumple satisfactoriamente con los requisitos de robustez, idempotencia y arquitectura solicitados, reconozco oportunidades valiosas para una mayor profundizaciÃ³n. De haber contado con mÃ¡s tiempo, el siguiente paso lÃ³gico hubiese sido una exploraciÃ³n mÃ¡s exhaustiva de los mÃºltiples endpoints de la API de RAWG, permitiendo realizar transformaciones lÃ³gicas mÃ¡s complejas e integrar modelos analÃ­ticos avanzados sobre los datos recolectados.

---
**Autor:** Daniel Arias
**Materia:** IngenierÃ­a de Datos - UTN
